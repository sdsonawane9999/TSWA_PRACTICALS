{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYMjQ9auOaKE"
   },
   "outputs": [],
   "source": [
    "# TSWA_PRACTICAL_7_TEXT_NORMALIZATION\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df4Ks_cROvCR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwdY7BhyO0Qh"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPrk2umGOk05"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUfTOQD8PPnu"
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqW_RlvuOPdr"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6Ra0Dn8OX5G"
   },
   "outputs": [],
   "source": [
    "def contraction_remover(text):\n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "    # using contractions.fix to expand the shortened words\n",
    "        expanded_words.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9g_2zomPgcE"
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(df):\n",
    "    # Remove HTML tags\n",
    "    df['Preprocess_Article'] = df['Article'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].str.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'http\\S+|www\\.\\S+', '', x))\n",
    "\n",
    "    # Remove email addresses\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove phone numbers\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\d{10}', '', x))\n",
    "\n",
    "    # Handle negation\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\bnot\\b(\\w+)', r'not_\\1', x))\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Remove numeric tokens\n",
    "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\b\\d+\\b', '', x))\n",
    "\n",
    "    # Tokenization\n",
    "    df['tokens'] = df['Preprocess_Article'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in x])\n",
    "\n",
    "     # Convert tokens into a single string\n",
    "    df['Clean Article'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F1nea9cZMn7"
   },
   "outputs": [],
   "source": [
    "# Helper function to map POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVqOUNZEQSEJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe7EwsNDQA8Y"
   },
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "data=fetch_20newsgroups(subset='all')\n",
    "data = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9rUtI1CQemB"
   },
   "outputs": [],
   "source": [
    "# Create objects for each package\n",
    "import numpy as np\n",
    "#import text_normalizer as tn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B69zflU-QamV"
   },
   "outputs": [],
   "source": [
    "# building the dataframe for the data extracted from newgroups\n",
    "# Create a corpus of newsgroup sentences and create the data frame\n",
    "corpus=data.data\n",
    "target_labels=data.target\n",
    "target_names = [data_labels_map[label] for label in data.target]\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YxplfbTyA__"
   },
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B89rX5QgSqor"
   },
   "outputs": [],
   "source": [
    "normalize_corpus(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP0wnustzRHQ"
   },
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pToY8qSW7t6P"
   },
   "outputs": [],
   "source": [
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd_NUmze79F8"
   },
   "outputs": [],
   "source": [
    "# Remove any unwanted characters\n",
    "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JaFOQGs8KwB"
   },
   "outputs": [],
   "source": [
    "data_df = data_df.dropna().reset_index(drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LXddnh68RjV"
   },
   "outputs": [],
   "source": [
    "# Creating a csv file of the cleaned doucment so that it can be reused\n",
    "data_df.to_csv('clean_newsgroups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIReOtmt9B1U"
   },
   "outputs": [],
   "source": [
    "# Data - training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfjJm8xW9GPi"
   },
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "np.array(data_df['Target Name']),test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV37Lv3l9Veu"
   },
   "outputs": [],
   "source": [
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGSTA2EH9fTZ"
   },
   "outputs": [],
   "source": [
    "# Create the dictionary for train and test data\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pchrhgAX9ifP"
   },
   "outputs": [],
   "source": [
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CswxNlvAQ3qN"
   },
   "outputs": [],
   "source": [
    "trd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_Sa_2YURE1R"
   },
   "outputs": [],
   "source": [
    "tsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzvlBk8V9mxL"
   },
   "outputs": [],
   "source": [
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd],columns=['Target Label', 'Train Count', 'Test Count'])\n",
    ".sort_values(by=['Train Count', 'Test Count'],ascending=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO7JxoOGSm2P/zrE3+ElWN",
   "mount_file_id": "1uBpMsZgYx5QqcMQQyMAyGR_tZzjbGcqX",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
